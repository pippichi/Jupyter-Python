{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5718e-22, 9.3887e-43, 1.5718e-22, 9.3887e-43, 1.5718e-22],\n",
       "        [9.3887e-43, 1.5718e-22, 9.3887e-43, 1.5718e-22, 9.3887e-43],\n",
       "        [1.5718e-22, 9.3887e-43, 1.5720e-22, 9.3887e-43, 1.5720e-22]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2413, 0.7775, 0.2731, 0.9709, 0.7671],\n",
       "        [0.6398, 0.4612, 0.3268, 0.9988, 0.7039],\n",
       "        [0.3963, 0.7648, 0.5971, 0.5713, 0.2478]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.zeros(3,5,dtype=torch.long)\n",
    "x = torch.zeros(3,5).long()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.0000, 5.5000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([3,5.5])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x.new_ones产生跟之前的x类型一样的全为1的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(5,3,dtype=torch.double)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6459, -0.0672, -1.6809],\n",
       "        [ 1.3108, -0.9414,  0.7365],\n",
       "        [-0.0344,  0.0642,  0.4711],\n",
       "        [-0.7241, -0.0651, -0.3972],\n",
       "        [ 1.0805, -0.3750,  0.4379]], dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn_like(x,dtype=float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0698,  0.7729, -1.4416],\n",
       "        [ 2.0220, -0.3866,  1.6293],\n",
       "        [ 0.2172,  0.2965,  1.0492],\n",
       "        [-0.0274,  0.3272,  0.2692],\n",
       "        [ 1.5777,  0.5648,  0.4684]], dtype=torch.float64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y\n",
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0698,  0.7729, -1.4416],\n",
       "        [ 2.0220, -0.3866,  1.6293],\n",
       "        [ 0.2172,  0.2965,  1.0492],\n",
       "        [-0.0274,  0.3272,  0.2692],\n",
       "        [ 1.5777,  0.5648,  0.4684]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.empty(5,3)\n",
    "torch.add(x,y,out=result)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .add_改变原矩阵的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0698,  0.7729, -1.4416],\n",
      "        [ 2.0220, -0.3866,  1.6293],\n",
      "        [ 0.2172,  0.2965,  1.0492],\n",
      "        [-0.0274,  0.3272,  0.2692],\n",
      "        [ 1.5777,  0.5648,  0.4684]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7157,  0.7058, -3.1226],\n",
       "        [ 3.3328, -1.3280,  2.3658],\n",
       "        [ 0.1828,  0.3607,  1.5203],\n",
       "        [-0.7515,  0.2620, -0.1280],\n",
       "        [ 2.6581,  0.1898,  0.9062]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y)\n",
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0672, -1.6809],\n",
       "        [-0.9414,  0.7365],\n",
       "        [ 0.0642,  0.4711],\n",
       "        [-0.0651, -0.3972],\n",
       "        [-0.3750,  0.4379]], dtype=torch.float64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resizing:numpy的reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2671,  0.1922],\n",
       "        [-0.5502, -0.0872],\n",
       "        [ 0.7656,  0.0113],\n",
       "        [ 0.0398,  0.2017],\n",
       "        [-0.3931, -0.6498],\n",
       "        [ 0.9086, -0.7373],\n",
       "        [ 1.2625, -1.0135],\n",
       "        [ 2.1627,  0.0720]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "y = x.view(16)\n",
    "z = x.view(8,-1) #-1的意思是让机器自动帮你算可划分成几列\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .item() 将tensor转化为python数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2835])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28348755836486816"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2671, -0.5502,  0.7656,  0.0398, -0.3931,  0.9086,  1.2625,  2.1627],\n",
       "        [ 0.1922, -0.0872,  0.0113,  0.2017, -0.6498, -0.7373, -1.0135,  0.0720]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor跟numpy共享内存，可以无缝切换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1] = 2\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 1., 1., 1.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a,1,out=a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a + 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显然上述例子的区别在于是否重新分配内存空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x,device=device)\n",
    "    x = x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\",torch.double))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpu上不能转numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.26712272,  0.19223848, -0.5502347 , -0.08716793,  0.7655558 ,\n",
       "        0.01134443,  0.03977102,  0.20174904, -0.39310893, -0.6497694 ,\n",
       "        0.90858555, -0.7373213 ,  1.2624683 , -1.0135039 ,  2.1626506 ,\n",
       "        0.07195156], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.to(\"cpu\").data.numpy()\n",
    "y.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.cuda() 把整个模型搬到gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    " - $h = W_1X + b_1$\n",
    " - $a = max(0,h)$\n",
    " - $y_{hat} = W_2a + b_2$\n",
    " \n",
    " 用numpy来计算前向神经网络，loss，和反向传播\n",
    " - forward pass\n",
    " - loss\n",
    " - backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "N,D_in,H,D_out = 64,1000,100,10\n",
    "\n",
    "#随机生成数据\n",
    "x = np.random.randn(N,D_in)\n",
    "y = np.random.randn(N,D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in,H)\n",
    "w2 = np.random.randn(H,D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for i in range(400):\n",
    "    h = x.dot(w1) #N,H\n",
    "    h_relu = np.maximum(h,0) #N,H\n",
    "    y_pred = h_relu.dot(w2) #N,D_out\n",
    "    \n",
    "    #complete loss\n",
    "    loss = np.square(y_pred - y).mean()\n",
    "    #print(loss)\n",
    "    \n",
    "    #后向传播\n",
    "    #计算梯度\n",
    "    grad_y_pred = 2.0 * (y_pred - y) #N,D_out\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred) #H,D_out\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T) #N,H\n",
    "    grad_h = grad_h_relu.copy() #N,H\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h) #D_in,H\n",
    "    \n",
    "    #更新w1，w2权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接下来将上述代码转换成pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28024786.0\n",
      "1 24068776.0\n",
      "2 23812446.0\n",
      "3 23888428.0\n",
      "4 22099734.0\n",
      "5 17946010.0\n",
      "6 12661491.0\n",
      "7 8026219.0\n",
      "8 4831643.5\n",
      "9 2940225.25\n",
      "10 1893410.5\n",
      "11 1317593.125\n",
      "12 987504.375\n",
      "13 784447.125\n",
      "14 648719.75\n",
      "15 550813.9375\n",
      "16 475771.375\n",
      "17 415737.15625\n",
      "18 366244.625\n",
      "19 324520.03125\n",
      "20 288880.90625\n",
      "21 258149.890625\n",
      "22 231453.296875\n",
      "23 208139.46875\n",
      "24 187472.640625\n",
      "25 169301.1875\n",
      "26 153269.734375\n",
      "27 139076.46875\n",
      "28 126473.78125\n",
      "29 115305.2421875\n",
      "30 105330.234375\n",
      "31 96398.859375\n",
      "32 88399.75\n",
      "33 81212.390625\n",
      "34 74733.953125\n",
      "35 68881.453125\n",
      "36 63571.7109375\n",
      "37 58752.515625\n",
      "38 54377.953125\n",
      "39 50392.6640625\n",
      "40 46756.109375\n",
      "41 43428.9765625\n",
      "42 40381.55078125\n",
      "43 37586.546875\n",
      "44 35019.47265625\n",
      "45 32659.380859375\n",
      "46 30486.96875\n",
      "47 28489.953125\n",
      "48 26644.412109375\n",
      "49 24937.443359375\n",
      "50 23356.484375\n",
      "51 21891.0625\n",
      "52 20532.12109375\n",
      "53 19270.451171875\n",
      "54 18096.9453125\n",
      "55 17004.857421875\n",
      "56 15987.416015625\n",
      "57 15038.8046875\n",
      "58 14153.5126953125\n",
      "59 13327.1025390625\n",
      "60 12555.1064453125\n",
      "61 11833.3759765625\n",
      "62 11157.7978515625\n",
      "63 10525.076171875\n",
      "64 9932.1298828125\n",
      "65 9376.0830078125\n",
      "66 8854.2685546875\n",
      "67 8364.4111328125\n",
      "68 7904.3583984375\n",
      "69 7471.865234375\n",
      "70 7065.52587890625\n",
      "71 6683.29541015625\n",
      "72 6323.4970703125\n",
      "73 5984.7529296875\n",
      "74 5665.8525390625\n",
      "75 5365.26416015625\n",
      "76 5081.7861328125\n",
      "77 4814.439453125\n",
      "78 4562.17626953125\n",
      "79 4324.154296875\n",
      "80 4099.60498046875\n",
      "81 3887.58642578125\n",
      "82 3687.367919921875\n",
      "83 3498.146240234375\n",
      "84 3319.260986328125\n",
      "85 3150.119140625\n",
      "86 2990.17626953125\n",
      "87 2838.824462890625\n",
      "88 2695.58935546875\n",
      "89 2559.949462890625\n",
      "90 2431.5166015625\n",
      "91 2309.892822265625\n",
      "92 2194.664306640625\n",
      "93 2085.492431640625\n",
      "94 1982.041259765625\n",
      "95 1883.980712890625\n",
      "96 1791.0062255859375\n",
      "97 1702.814697265625\n",
      "98 1619.206787109375\n",
      "99 1539.9088134765625\n",
      "100 1464.634033203125\n",
      "101 1393.203857421875\n",
      "102 1325.4044189453125\n",
      "103 1261.05712890625\n",
      "104 1199.9735107421875\n",
      "105 1141.9530029296875\n",
      "106 1086.850830078125\n",
      "107 1034.5208740234375\n",
      "108 984.7999267578125\n",
      "109 937.5714721679688\n",
      "110 892.7012329101562\n",
      "111 850.0388793945312\n",
      "112 809.4837646484375\n",
      "113 770.9561767578125\n",
      "114 734.3250122070312\n",
      "115 699.4815063476562\n",
      "116 666.354736328125\n",
      "117 634.8568115234375\n",
      "118 604.8870849609375\n",
      "119 576.3823852539062\n",
      "120 549.2650756835938\n",
      "121 523.4615478515625\n",
      "122 498.9087219238281\n",
      "123 475.5449523925781\n",
      "124 453.31060791015625\n",
      "125 432.14642333984375\n",
      "126 411.99407958984375\n",
      "127 392.8066101074219\n",
      "128 374.5413513183594\n",
      "129 357.1501770019531\n",
      "130 340.5963134765625\n",
      "131 324.8232421875\n",
      "132 309.79901123046875\n",
      "133 295.4872131347656\n",
      "134 281.8550109863281\n",
      "135 268.8642578125\n",
      "136 256.4922180175781\n",
      "137 244.70123291015625\n",
      "138 233.4671630859375\n",
      "139 222.75889587402344\n",
      "140 212.55763244628906\n",
      "141 202.83322143554688\n",
      "142 193.56358337402344\n",
      "143 184.7267608642578\n",
      "144 176.3056182861328\n",
      "145 168.27517700195312\n",
      "146 160.61947631835938\n",
      "147 153.31802368164062\n",
      "148 146.3589630126953\n",
      "149 139.721435546875\n",
      "150 133.38938903808594\n",
      "151 127.35205078125\n",
      "152 121.59235382080078\n",
      "153 116.10184478759766\n",
      "154 110.86077117919922\n",
      "155 105.86265563964844\n",
      "156 101.09416961669922\n",
      "157 96.5461654663086\n",
      "158 92.20623016357422\n",
      "159 88.06378936767578\n",
      "160 84.1119613647461\n",
      "161 80.34088897705078\n",
      "162 76.74382019042969\n",
      "163 73.30987548828125\n",
      "164 70.03152465820312\n",
      "165 66.90258026123047\n",
      "166 63.916690826416016\n",
      "167 61.06764602661133\n",
      "168 58.3494758605957\n",
      "169 55.75294494628906\n",
      "170 53.27488327026367\n",
      "171 50.907989501953125\n",
      "172 48.64879608154297\n",
      "173 46.491641998291016\n",
      "174 44.43240737915039\n",
      "175 42.46585464477539\n",
      "176 40.58733367919922\n",
      "177 38.793548583984375\n",
      "178 37.07950973510742\n",
      "179 35.4434814453125\n",
      "180 33.879966735839844\n",
      "181 32.38699722290039\n",
      "182 30.960857391357422\n",
      "183 29.599117279052734\n",
      "184 28.29789161682129\n",
      "185 27.05463981628418\n",
      "186 25.866180419921875\n",
      "187 24.73189353942871\n",
      "188 23.647903442382812\n",
      "189 22.612049102783203\n",
      "190 21.622852325439453\n",
      "191 20.676877975463867\n",
      "192 19.773080825805664\n",
      "193 18.90939712524414\n",
      "194 18.083375930786133\n",
      "195 17.294538497924805\n",
      "196 16.543811798095703\n",
      "197 15.826397895812988\n",
      "198 15.13985824584961\n",
      "199 14.484479904174805\n",
      "200 13.857007026672363\n",
      "201 13.257789611816406\n",
      "202 12.684622764587402\n",
      "203 12.13721752166748\n",
      "204 11.61284065246582\n",
      "205 11.111991882324219\n",
      "206 10.633109092712402\n",
      "207 10.174824714660645\n",
      "208 9.736862182617188\n",
      "209 9.317877769470215\n",
      "210 8.917242050170898\n",
      "211 8.534103393554688\n",
      "212 8.167901992797852\n",
      "213 7.817074775695801\n",
      "214 7.481626987457275\n",
      "215 7.161196231842041\n",
      "216 6.854109287261963\n",
      "217 6.560737609863281\n",
      "218 6.280020713806152\n",
      "219 6.0114946365356445\n",
      "220 5.754356861114502\n",
      "221 5.508853435516357\n",
      "222 5.273456573486328\n",
      "223 5.048460483551025\n",
      "224 4.833189964294434\n",
      "225 4.627127170562744\n",
      "226 4.429886341094971\n",
      "227 4.241203308105469\n",
      "228 4.060779094696045\n",
      "229 3.8878333568573\n",
      "230 3.7226030826568604\n",
      "231 3.56463623046875\n",
      "232 3.413238763809204\n",
      "233 3.26835560798645\n",
      "234 3.129617214202881\n",
      "235 2.996800184249878\n",
      "236 2.8697800636291504\n",
      "237 2.7483325004577637\n",
      "238 2.6318633556365967\n",
      "239 2.5205321311950684\n",
      "240 2.4140632152557373\n",
      "241 2.311845541000366\n",
      "242 2.21419358253479\n",
      "243 2.1207683086395264\n",
      "244 2.0312275886535645\n",
      "245 1.9455158710479736\n",
      "246 1.8634746074676514\n",
      "247 1.7850651741027832\n",
      "248 1.709814190864563\n",
      "249 1.6377215385437012\n",
      "250 1.568795919418335\n",
      "251 1.5028016567230225\n",
      "252 1.4395678043365479\n",
      "253 1.3791576623916626\n",
      "254 1.321237564086914\n",
      "255 1.2657220363616943\n",
      "256 1.2126199007034302\n",
      "257 1.1616626977920532\n",
      "258 1.1130547523498535\n",
      "259 1.0663100481033325\n",
      "260 1.0215413570404053\n",
      "261 0.9788969159126282\n",
      "262 0.9379609227180481\n",
      "263 0.898629367351532\n",
      "264 0.8611091375350952\n",
      "265 0.8250932097434998\n",
      "266 0.7906408309936523\n",
      "267 0.7575618028640747\n",
      "268 0.7259682416915894\n",
      "269 0.6956908106803894\n",
      "270 0.66661137342453\n",
      "271 0.6388994455337524\n",
      "272 0.6122568845748901\n",
      "273 0.5867301225662231\n",
      "274 0.5623461008071899\n",
      "275 0.5390530228614807\n",
      "276 0.5164849758148193\n",
      "277 0.49496957659721375\n",
      "278 0.4744509160518646\n",
      "279 0.454728364944458\n",
      "280 0.43591582775115967\n",
      "281 0.4177577495574951\n",
      "282 0.4004225432872772\n",
      "283 0.38381630182266235\n",
      "284 0.36794617772102356\n",
      "285 0.352641761302948\n",
      "286 0.33805596828460693\n",
      "287 0.3240518569946289\n",
      "288 0.3106769919395447\n",
      "289 0.2978166341781616\n",
      "290 0.28548920154571533\n",
      "291 0.27370205521583557\n",
      "292 0.2623271346092224\n",
      "293 0.2514922618865967\n",
      "294 0.2411143183708191\n",
      "295 0.23116305470466614\n",
      "296 0.22160297632217407\n",
      "297 0.21249671280384064\n",
      "298 0.20371945202350616\n",
      "299 0.19532348215579987\n",
      "300 0.18727293610572815\n",
      "301 0.17953462898731232\n",
      "302 0.17214542627334595\n",
      "303 0.1650812327861786\n",
      "304 0.15830276906490326\n",
      "305 0.15180473029613495\n",
      "306 0.14558763802051544\n",
      "307 0.13963080942630768\n",
      "308 0.13387368619441986\n",
      "309 0.1283399909734726\n",
      "310 0.12309803068637848\n",
      "311 0.11803058534860611\n",
      "312 0.1132429987192154\n",
      "313 0.10857977718114853\n",
      "314 0.1041138619184494\n",
      "315 0.09986590594053268\n",
      "316 0.09581154584884644\n",
      "317 0.09186451137065887\n",
      "318 0.08811356872320175\n",
      "319 0.08448892086744308\n",
      "320 0.08103921264410019\n",
      "321 0.07775977998971939\n",
      "322 0.0745776817202568\n",
      "323 0.07152377814054489\n",
      "324 0.06864268332719803\n",
      "325 0.06584059447050095\n",
      "326 0.06315519660711288\n",
      "327 0.060580525547266006\n",
      "328 0.058113351464271545\n",
      "329 0.055718809366226196\n",
      "330 0.05345572531223297\n",
      "331 0.05130065605044365\n",
      "332 0.04923499375581741\n",
      "333 0.04723595082759857\n",
      "334 0.04531821608543396\n",
      "335 0.04348549246788025\n",
      "336 0.04171914607286453\n",
      "337 0.040025051683187485\n",
      "338 0.0383986197412014\n",
      "339 0.03685947135090828\n",
      "340 0.03535711020231247\n",
      "341 0.033923324197530746\n",
      "342 0.03255290538072586\n",
      "343 0.031248128041625023\n",
      "344 0.029985560104250908\n",
      "345 0.028780411928892136\n",
      "346 0.027601569890975952\n",
      "347 0.02650373987853527\n",
      "348 0.025424150750041008\n",
      "349 0.024412669241428375\n",
      "350 0.023432882502675056\n",
      "351 0.022499576210975647\n",
      "352 0.021599112078547478\n",
      "353 0.02073291689157486\n",
      "354 0.019904350861907005\n",
      "355 0.0191070307046175\n",
      "356 0.018356449902057648\n",
      "357 0.01761842705309391\n",
      "358 0.016914715990424156\n",
      "359 0.016238970682024956\n",
      "360 0.015596605837345123\n",
      "361 0.014976413920521736\n",
      "362 0.014372587203979492\n",
      "363 0.013806462287902832\n",
      "364 0.013256176374852657\n",
      "365 0.012742767110466957\n",
      "366 0.012240549549460411\n",
      "367 0.011763055808842182\n",
      "368 0.011302297934889793\n",
      "369 0.010864276438951492\n",
      "370 0.010436365380883217\n",
      "371 0.010026970878243446\n",
      "372 0.009633621200919151\n",
      "373 0.00926252268254757\n",
      "374 0.008910173550248146\n",
      "375 0.008569225668907166\n",
      "376 0.008235270157456398\n",
      "377 0.007915973663330078\n",
      "378 0.007612226530909538\n",
      "379 0.0073162224143743515\n",
      "380 0.007039422634989023\n",
      "381 0.006770993582904339\n",
      "382 0.006509521044790745\n",
      "383 0.006265593692660332\n",
      "384 0.006033619865775108\n",
      "385 0.0058027394115924835\n",
      "386 0.005586039740592241\n",
      "387 0.0053738756105303764\n",
      "388 0.005169655662029982\n",
      "389 0.004978008568286896\n",
      "390 0.004797943402081728\n",
      "391 0.0046225907281041145\n",
      "392 0.00444851303473115\n",
      "393 0.00428415322676301\n",
      "394 0.004125860054045916\n",
      "395 0.003977098967880011\n",
      "396 0.003832519054412842\n",
      "397 0.0036925191525369883\n",
      "398 0.003558824071660638\n",
      "399 0.0034315716475248337\n"
     ]
    }
   ],
   "source": [
    "N,D_in,H,D_out = 64,1000,100,10\n",
    "\n",
    "#在gpu上：x = torch.randn(N,D_in).to(\"cuda:0\")\n",
    "#或者：x = torch.randn(N,D_in).cuda()\n",
    "\n",
    "x = torch.randn(N,D_in)\n",
    "y = torch.randn(N,D_out)\n",
    "\n",
    "# w1 = torch.randn(D_in,H,requires_grad=True)\n",
    "# w2 = torch.randn(H,D_out,requires_grad=True)\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in,H,bias=False), #w_1 * x + b_1\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H,D_out,bias=False),\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for i in range(400):\n",
    "#     h = x.mm(w1)\n",
    "#     h_relu = h.clamp(min=0) #这里还可以加一个max = ？的参数表示上线夹到哪\n",
    "#     y_pred = h_relu.mm(w2)\n",
    "#     或y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    y_pred = model(x) #model.forward()\n",
    "    \n",
    "#     loss = (y_pred - y).pow(2).sum()\n",
    "    loss = loss_fn(y_pred,y)\n",
    "    print(i,loss.item())\n",
    "    \n",
    "    #grad重置0，不然每一次都会叠加\n",
    "    model.zero_grad()\n",
    "    \n",
    "#     grad_y_pred = 2.0*(y_pred - y)\n",
    "#     grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "#     grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "#     grad_h = grad_h_relu.clone()\n",
    "#     grad_h[h<0] = 0\n",
    "#     grad_w1 = x.t().mm(grad_h)\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#         w1 -= learning_rate * w1.grad\n",
    "#         w2 -= learning_rate * w2.grad\n",
    "#         w1.grad.zero_()\n",
    "#         w2.grad.zero_()\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1000, out_features=100, bias=False)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0198,  0.0255, -0.0180,  ...,  0.0166,  0.0119, -0.0055],\n",
       "        [ 0.0027, -0.0206, -0.0258,  ...,  0.0084,  0.0282,  0.0250],\n",
       "        [ 0.0228,  0.0179,  0.0133,  ..., -0.0018, -0.0132,  0.0122],\n",
       "        ...,\n",
       "        [-0.0109,  0.0022,  0.0045,  ..., -0.0291,  0.0071, -0.0115],\n",
       "        [ 0.0025,  0.0231,  0.0422,  ...,  0.0097, -0.0104,  0.0109],\n",
       "        [-0.0206,  0.0181,  0.0021,  ..., -0.0222,  0.0096,  0.0196]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单的autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(2.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.,requires_grad=True)\n",
    "w = torch.tensor(2.,requires_grad=True)\n",
    "b = torch.tensor(3.,requires_grad=True)\n",
    "\n",
    "y = w*x + b \n",
    "\n",
    "y.backward()\n",
    "\n",
    "# dy/dw = x\n",
    "print(w.grad)\n",
    "print(x.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 608.7940673828125\n",
      "1 565.3714599609375\n",
      "2 527.3673706054688\n",
      "3 494.01983642578125\n",
      "4 463.9308776855469\n",
      "5 436.6506042480469\n",
      "6 411.97119140625\n",
      "7 389.25726318359375\n",
      "8 368.3084716796875\n",
      "9 349.0548095703125\n",
      "10 331.09796142578125\n",
      "11 314.2249755859375\n",
      "12 298.23779296875\n",
      "13 283.14556884765625\n",
      "14 268.80194091796875\n",
      "15 255.19915771484375\n",
      "16 242.29791259765625\n",
      "17 230.08290100097656\n",
      "18 218.42391967773438\n",
      "19 207.27566528320312\n",
      "20 196.6880340576172\n",
      "21 186.60382080078125\n",
      "22 176.98373413085938\n",
      "23 167.79808044433594\n",
      "24 159.029052734375\n",
      "25 150.6370086669922\n",
      "26 142.621826171875\n",
      "27 134.96022033691406\n",
      "28 127.62770080566406\n",
      "29 120.65863037109375\n",
      "30 113.98528289794922\n",
      "31 107.63106536865234\n",
      "32 101.5916976928711\n",
      "33 95.84913635253906\n",
      "34 90.4012451171875\n",
      "35 85.24464416503906\n",
      "36 80.38154602050781\n",
      "37 75.7739486694336\n",
      "38 71.41039276123047\n",
      "39 67.28842163085938\n",
      "40 63.402984619140625\n",
      "41 59.73355484008789\n",
      "42 56.27155685424805\n",
      "43 53.00912857055664\n",
      "44 49.94029235839844\n",
      "45 47.05471420288086\n",
      "46 44.34104919433594\n",
      "47 41.790313720703125\n",
      "48 39.388511657714844\n",
      "49 37.13324737548828\n",
      "50 35.01448440551758\n",
      "51 33.02459716796875\n",
      "52 31.156578063964844\n",
      "53 29.401813507080078\n",
      "54 27.754066467285156\n",
      "55 26.2047061920166\n",
      "56 24.750017166137695\n",
      "57 23.382850646972656\n",
      "58 22.0985107421875\n",
      "59 20.89093017578125\n",
      "60 19.75691795349121\n",
      "61 18.691137313842773\n",
      "62 17.690141677856445\n",
      "63 16.748493194580078\n",
      "64 15.860486030578613\n",
      "65 15.02405834197998\n",
      "66 14.235610961914062\n",
      "67 13.492738723754883\n",
      "68 12.793549537658691\n",
      "69 12.135024070739746\n",
      "70 11.51318645477295\n",
      "71 10.92544937133789\n",
      "72 10.37098217010498\n",
      "73 9.847573280334473\n",
      "74 9.353384017944336\n",
      "75 8.886314392089844\n",
      "76 8.444930076599121\n",
      "77 8.026908874511719\n",
      "78 7.63206672668457\n",
      "79 7.257537364959717\n",
      "80 6.903163433074951\n",
      "81 6.56788969039917\n",
      "82 6.250570774078369\n",
      "83 5.950314521789551\n",
      "84 5.6656646728515625\n",
      "85 5.396020889282227\n",
      "86 5.14077091217041\n",
      "87 4.898863315582275\n",
      "88 4.669101238250732\n",
      "89 4.451066493988037\n",
      "90 4.24398946762085\n",
      "91 4.047203540802002\n",
      "92 3.8596138954162598\n",
      "93 3.6814804077148438\n",
      "94 3.512176275253296\n",
      "95 3.3512942790985107\n",
      "96 3.1985836029052734\n",
      "97 3.0532777309417725\n",
      "98 2.915203332901001\n",
      "99 2.783801794052124\n",
      "100 2.658790111541748\n",
      "101 2.5396952629089355\n",
      "102 2.4263994693756104\n",
      "103 2.318567991256714\n",
      "104 2.2157912254333496\n",
      "105 2.117931365966797\n",
      "106 2.0246472358703613\n",
      "107 1.9358019828796387\n",
      "108 1.851191759109497\n",
      "109 1.7704659700393677\n",
      "110 1.6934752464294434\n",
      "111 1.6203967332839966\n",
      "112 1.5507668256759644\n",
      "113 1.4844326972961426\n",
      "114 1.4211540222167969\n",
      "115 1.3607678413391113\n",
      "116 1.3031278848648071\n",
      "117 1.2481082677841187\n",
      "118 1.1956089735031128\n",
      "119 1.1454923152923584\n",
      "120 1.0977752208709717\n",
      "121 1.0522809028625488\n",
      "122 1.0088226795196533\n",
      "123 0.9673700332641602\n",
      "124 0.9277766942977905\n",
      "125 0.8899325728416443\n",
      "126 0.8537886738777161\n",
      "127 0.8192220330238342\n",
      "128 0.7861764430999756\n",
      "129 0.7545788288116455\n",
      "130 0.7243819832801819\n",
      "131 0.695502519607544\n",
      "132 0.6678194999694824\n",
      "133 0.6412339806556702\n",
      "134 0.6158540844917297\n",
      "135 0.591529130935669\n",
      "136 0.5682458281517029\n",
      "137 0.5459338426589966\n",
      "138 0.5245748162269592\n",
      "139 0.5041086673736572\n",
      "140 0.48450613021850586\n",
      "141 0.46571779251098633\n",
      "142 0.44770291447639465\n",
      "143 0.4304535984992981\n",
      "144 0.4139028787612915\n",
      "145 0.39805877208709717\n",
      "146 0.38285696506500244\n",
      "147 0.36828893423080444\n",
      "148 0.35430166125297546\n",
      "149 0.34088897705078125\n",
      "150 0.32802295684814453\n",
      "151 0.3156861662864685\n",
      "152 0.30383938550949097\n",
      "153 0.2924819886684418\n",
      "154 0.2815774083137512\n",
      "155 0.27110058069229126\n",
      "156 0.261041522026062\n",
      "157 0.25138914585113525\n",
      "158 0.24213069677352905\n",
      "159 0.2332284152507782\n",
      "160 0.22468116879463196\n",
      "161 0.2164665162563324\n",
      "162 0.20857711136341095\n",
      "163 0.20099332928657532\n",
      "164 0.1937059611082077\n",
      "165 0.1867053061723709\n",
      "166 0.1799798160791397\n",
      "167 0.17351417243480682\n",
      "168 0.1672917604446411\n",
      "169 0.161306232213974\n",
      "170 0.15556740760803223\n",
      "171 0.15003439784049988\n",
      "172 0.14471429586410522\n",
      "173 0.13959495723247528\n",
      "174 0.13466967642307281\n",
      "175 0.1299295574426651\n",
      "176 0.125369593501091\n",
      "177 0.1209796890616417\n",
      "178 0.11675296723842621\n",
      "179 0.11268594861030579\n",
      "180 0.1087716668844223\n",
      "181 0.10500660538673401\n",
      "182 0.10137861222028732\n",
      "183 0.09788487106561661\n",
      "184 0.0945194810628891\n",
      "185 0.09127823263406754\n",
      "186 0.0881604552268982\n",
      "187 0.08515876531600952\n",
      "188 0.08226682245731354\n",
      "189 0.07947780936956406\n",
      "190 0.07679189741611481\n",
      "191 0.07420387864112854\n",
      "192 0.07171262800693512\n",
      "193 0.06931118667125702\n",
      "194 0.066993348300457\n",
      "195 0.06475871801376343\n",
      "196 0.06260428577661514\n",
      "197 0.06052803620696068\n",
      "198 0.058525506407022476\n",
      "199 0.056594520807266235\n",
      "200 0.054731257259845734\n",
      "201 0.05293547362089157\n",
      "202 0.05120569467544556\n",
      "203 0.04953407496213913\n",
      "204 0.047919947654008865\n",
      "205 0.04636285826563835\n",
      "206 0.04486009106040001\n",
      "207 0.04340965300798416\n",
      "208 0.042009226977825165\n",
      "209 0.04065720736980438\n",
      "210 0.03935170918703079\n",
      "211 0.03809245303273201\n",
      "212 0.03687542676925659\n",
      "213 0.035699985921382904\n",
      "214 0.034565165638923645\n",
      "215 0.03346945345401764\n",
      "216 0.03241109475493431\n",
      "217 0.031388889998197556\n",
      "218 0.030400265008211136\n",
      "219 0.02944604493677616\n",
      "220 0.028525423258543015\n",
      "221 0.027633965015411377\n",
      "222 0.02677387371659279\n",
      "223 0.025941574946045876\n",
      "224 0.02513708360493183\n",
      "225 0.024359453469514847\n",
      "226 0.023607930168509483\n",
      "227 0.022880952805280685\n",
      "228 0.02217811346054077\n",
      "229 0.021498659625649452\n",
      "230 0.020841019228100777\n",
      "231 0.020205603912472725\n",
      "232 0.019591286778450012\n",
      "233 0.018996471539139748\n",
      "234 0.018421076238155365\n",
      "235 0.01786460168659687\n",
      "236 0.01732613518834114\n",
      "237 0.016804948449134827\n",
      "238 0.016300663352012634\n",
      "239 0.015813004225492477\n",
      "240 0.015340682119131088\n",
      "241 0.014883872121572495\n",
      "242 0.014442486688494682\n",
      "243 0.014014537446200848\n",
      "244 0.013599914498627186\n",
      "245 0.013199309818446636\n",
      "246 0.012811228632926941\n",
      "247 0.012435045093297958\n",
      "248 0.01207052357494831\n",
      "249 0.011717395856976509\n",
      "250 0.011375565081834793\n",
      "251 0.01104443147778511\n",
      "252 0.010723834857344627\n",
      "253 0.0104131530970335\n",
      "254 0.010111994110047817\n",
      "255 0.009820216335356236\n",
      "256 0.009537409991025925\n",
      "257 0.009263383224606514\n",
      "258 0.008998032659292221\n",
      "259 0.008740694262087345\n",
      "260 0.008491290733218193\n",
      "261 0.008249874226748943\n",
      "262 0.008015836589038372\n",
      "263 0.00778868468478322\n",
      "264 0.0075683025643229485\n",
      "265 0.007354705594480038\n",
      "266 0.00714768934994936\n",
      "267 0.006946820765733719\n",
      "268 0.006751943379640579\n",
      "269 0.006563173606991768\n",
      "270 0.006379886530339718\n",
      "271 0.0062022400088608265\n",
      "272 0.006029886193573475\n",
      "273 0.005862649530172348\n",
      "274 0.005700371693819761\n",
      "275 0.005543016828596592\n",
      "276 0.005390196572989225\n",
      "277 0.005241879262030125\n",
      "278 0.005098165012896061\n",
      "279 0.004958702716976404\n",
      "280 0.004823153838515282\n",
      "281 0.004691643640398979\n",
      "282 0.004563964903354645\n",
      "283 0.004439991433173418\n",
      "284 0.004319598898291588\n",
      "285 0.004202731419354677\n",
      "286 0.004089293535798788\n",
      "287 0.003979075700044632\n",
      "288 0.003872083965688944\n",
      "289 0.0037681320682168007\n",
      "290 0.0036671715788543224\n",
      "291 0.0035691550001502037\n",
      "292 0.0034739100374281406\n",
      "293 0.003381389658898115\n",
      "294 0.0032917927019298077\n",
      "295 0.0032044844701886177\n",
      "296 0.003119636792689562\n",
      "297 0.003037184476852417\n",
      "298 0.0029571070335805416\n",
      "299 0.0028792601078748703\n",
      "300 0.0028035857249051332\n",
      "301 0.002730081556364894\n",
      "302 0.0026585960295051336\n",
      "303 0.002589106559753418\n",
      "304 0.0025216015055775642\n",
      "305 0.0024559583980590105\n",
      "306 0.002392148831859231\n",
      "307 0.0023300761822611094\n",
      "308 0.002269878750666976\n",
      "309 0.002211266430094838\n",
      "310 0.0021543598268181086\n",
      "311 0.0020990357734262943\n",
      "312 0.002045055152848363\n",
      "313 0.0019925846718251705\n",
      "314 0.0019415403949096799\n",
      "315 0.0018918924033641815\n",
      "316 0.0018435929669067264\n",
      "317 0.001796639058738947\n",
      "318 0.0017508836463093758\n",
      "319 0.0017064015846699476\n",
      "320 0.001663115224801004\n",
      "321 0.0016210789326578379\n",
      "322 0.0015801151748746634\n",
      "323 0.0015402463031932712\n",
      "324 0.0015014244709163904\n",
      "325 0.0014636588748544455\n",
      "326 0.001426894566975534\n",
      "327 0.0013911211863160133\n",
      "328 0.0013562936801463366\n",
      "329 0.0013223993591964245\n",
      "330 0.0012893960811197758\n",
      "331 0.0012572703417390585\n",
      "332 0.0012259777868166566\n",
      "333 0.0011955708032473922\n",
      "334 0.0011659422889351845\n",
      "335 0.0011370510328561068\n",
      "336 0.001108938013203442\n",
      "337 0.0010815388523042202\n",
      "338 0.0010548766003921628\n",
      "339 0.0010288968915119767\n",
      "340 0.0010035950690507889\n",
      "341 0.0009789579780772328\n",
      "342 0.0009549540118314326\n",
      "343 0.0009315762436017394\n",
      "344 0.0009088197839446366\n",
      "345 0.0008866803254932165\n",
      "346 0.0008650458767078817\n",
      "347 0.0008439923403784633\n",
      "348 0.0008234583074226975\n",
      "349 0.0008034608908928931\n",
      "350 0.0007839805330149829\n",
      "351 0.0007649938343092799\n",
      "352 0.0007464832160621881\n",
      "353 0.0007284663151949644\n",
      "354 0.000710877706296742\n",
      "355 0.0006937570869922638\n",
      "356 0.0006770952022634447\n",
      "357 0.000660816440358758\n",
      "358 0.0006449506036005914\n",
      "359 0.000629473477602005\n",
      "360 0.0006143979262560606\n",
      "361 0.000599693797994405\n",
      "362 0.0005853659240528941\n",
      "363 0.0005714003928005695\n",
      "364 0.0005577726988121867\n",
      "365 0.0005445130518637598\n",
      "366 0.0005315857706591487\n",
      "367 0.000518952205311507\n",
      "368 0.000506638316437602\n",
      "369 0.0004946301924064755\n",
      "370 0.0004829179379157722\n",
      "371 0.00047149977763183415\n",
      "372 0.00046035400009714067\n",
      "373 0.0004494899185374379\n",
      "374 0.00043889295193366706\n",
      "375 0.0004285890026949346\n",
      "376 0.00041850717389024794\n",
      "377 0.00040866920608095825\n",
      "378 0.0003990771365351975\n",
      "379 0.0003897158894687891\n",
      "380 0.00038058700738474727\n",
      "381 0.00037162259104661644\n",
      "382 0.00036288725095801055\n",
      "383 0.00035436515463516116\n",
      "384 0.0003460735024418682\n",
      "385 0.0003379647387191653\n",
      "386 0.00033005481236614287\n",
      "387 0.0003223391540814191\n",
      "388 0.00031480734469369054\n",
      "389 0.00030746348784305155\n",
      "390 0.0003002923622261733\n",
      "391 0.00029330249526537955\n",
      "392 0.00028648044099099934\n",
      "393 0.00027983661857433617\n",
      "394 0.00027333913021720946\n",
      "395 0.0002670014218892902\n",
      "396 0.00026081292890012264\n",
      "397 0.0002547760959714651\n",
      "398 0.00024888443294912577\n",
      "399 0.00024312682216987014\n"
     ]
    }
   ],
   "source": [
    "N,D_in,H,D_out = 64,1000,100,10\n",
    "\n",
    "x = torch.randn(N,D_in)\n",
    "y = torch.randn(N,D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in,H,bias=False), #w_1 * x + b_1\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H,D_out,bias=False),\n",
    ")\n",
    "\n",
    "# #数据初始化normal\n",
    "# torch.nn.init.normal_(model[0].weight)\n",
    "# torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for i in range(400):\n",
    "    y_pred = model(x) #model.forward()\n",
    "    \n",
    "    loss = loss_fn(y_pred,y)\n",
    "    print(i,loss.item())\n",
    "    \n",
    "#     #grad重置0，不然每一次都会叠加\n",
    "#     model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for param in model.parameters():\n",
    "#             param -= learning_rate * param.grad\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用pytorch里面的模型建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 692.671142578125\n",
      "1 675.111083984375\n",
      "2 658.0176391601562\n",
      "3 641.4360961914062\n",
      "4 625.4054565429688\n",
      "5 609.8909301757812\n",
      "6 594.8165283203125\n",
      "7 580.1303100585938\n",
      "8 565.884033203125\n",
      "9 552.0692138671875\n",
      "10 538.7452392578125\n",
      "11 525.826171875\n",
      "12 513.2382202148438\n",
      "13 501.032958984375\n",
      "14 489.1074523925781\n",
      "15 477.52691650390625\n",
      "16 466.2457275390625\n",
      "17 455.23089599609375\n",
      "18 444.5205078125\n",
      "19 434.14154052734375\n",
      "20 424.06317138671875\n",
      "21 414.2850646972656\n",
      "22 404.7872314453125\n",
      "23 395.5309753417969\n",
      "24 386.5009765625\n",
      "25 377.67291259765625\n",
      "26 369.05615234375\n",
      "27 360.6259460449219\n",
      "28 352.4234313964844\n",
      "29 344.39630126953125\n",
      "30 336.58221435546875\n",
      "31 328.9598693847656\n",
      "32 321.4980773925781\n",
      "33 314.21246337890625\n",
      "34 307.07293701171875\n",
      "35 300.0696716308594\n",
      "36 293.20721435546875\n",
      "37 286.479736328125\n",
      "38 279.93988037109375\n",
      "39 273.53643798828125\n",
      "40 267.2429504394531\n",
      "41 261.0813903808594\n",
      "42 255.05245971679688\n",
      "43 249.12167358398438\n",
      "44 243.315185546875\n",
      "45 237.63018798828125\n",
      "46 232.05740356445312\n",
      "47 226.58047485351562\n",
      "48 221.226318359375\n",
      "49 215.9888916015625\n",
      "50 210.86593627929688\n",
      "51 205.86122131347656\n",
      "52 200.9515838623047\n",
      "53 196.15589904785156\n",
      "54 191.45103454589844\n",
      "55 186.84872436523438\n",
      "56 182.34323120117188\n",
      "57 177.93406677246094\n",
      "58 173.60850524902344\n",
      "59 169.3739776611328\n",
      "60 165.22731018066406\n",
      "61 161.1732635498047\n",
      "62 157.19851684570312\n",
      "63 153.3007049560547\n",
      "64 149.48733520507812\n",
      "65 145.7499542236328\n",
      "66 142.09298706054688\n",
      "67 138.51351928710938\n",
      "68 135.0152587890625\n",
      "69 131.5957794189453\n",
      "70 128.23939514160156\n",
      "71 124.94983673095703\n",
      "72 121.7339096069336\n",
      "73 118.58020782470703\n",
      "74 115.49758911132812\n",
      "75 112.48692321777344\n",
      "76 109.54411315917969\n",
      "77 106.67015075683594\n",
      "78 103.85662841796875\n",
      "79 101.09893798828125\n",
      "80 98.40359497070312\n",
      "81 95.77420043945312\n",
      "82 93.20679473876953\n",
      "83 90.69779205322266\n",
      "84 88.25006866455078\n",
      "85 85.8595199584961\n",
      "86 83.52926635742188\n",
      "87 81.25141906738281\n",
      "88 79.02367401123047\n",
      "89 76.84913635253906\n",
      "90 74.72013854980469\n",
      "91 72.64263916015625\n",
      "92 70.61077880859375\n",
      "93 68.62590026855469\n",
      "94 66.68452453613281\n",
      "95 64.79037475585938\n",
      "96 62.942718505859375\n",
      "97 61.13774490356445\n",
      "98 59.376800537109375\n",
      "99 57.659584045410156\n",
      "100 55.98116683959961\n",
      "101 54.34161376953125\n",
      "102 52.742576599121094\n",
      "103 51.18475341796875\n",
      "104 49.664215087890625\n",
      "105 48.17937088012695\n",
      "106 46.73424530029297\n",
      "107 45.32467269897461\n",
      "108 43.952362060546875\n",
      "109 42.6148567199707\n",
      "110 41.31428527832031\n",
      "111 40.04474639892578\n",
      "112 38.80923080444336\n",
      "113 37.606266021728516\n",
      "114 36.43547821044922\n",
      "115 35.294918060302734\n",
      "116 34.18560791015625\n",
      "117 33.10515213012695\n",
      "118 32.053466796875\n",
      "119 31.03126335144043\n",
      "120 30.037330627441406\n",
      "121 29.069936752319336\n",
      "122 28.125568389892578\n",
      "123 27.20416259765625\n",
      "124 26.307003021240234\n",
      "125 25.433189392089844\n",
      "126 24.583227157592773\n",
      "127 23.75501823425293\n",
      "128 22.949142456054688\n",
      "129 22.166593551635742\n",
      "130 21.407360076904297\n",
      "131 20.669069290161133\n",
      "132 19.9518985748291\n",
      "133 19.255704879760742\n",
      "134 18.578643798828125\n",
      "135 17.921466827392578\n",
      "136 17.283348083496094\n",
      "137 16.663785934448242\n",
      "138 16.06256103515625\n",
      "139 15.479361534118652\n",
      "140 14.913806915283203\n",
      "141 14.36641788482666\n",
      "142 13.83543586730957\n",
      "143 13.321528434753418\n",
      "144 12.824311256408691\n",
      "145 12.343624114990234\n",
      "146 11.878390312194824\n",
      "147 11.429008483886719\n",
      "148 10.994906425476074\n",
      "149 10.575881004333496\n",
      "150 10.170268058776855\n",
      "151 9.778801918029785\n",
      "152 9.400823593139648\n",
      "153 9.035676002502441\n",
      "154 8.683391571044922\n",
      "155 8.343596458435059\n",
      "156 8.0155668258667\n",
      "157 7.699238300323486\n",
      "158 7.3944926261901855\n",
      "159 7.101315498352051\n",
      "160 6.818714141845703\n",
      "161 6.546609401702881\n",
      "162 6.284355163574219\n",
      "163 6.032066345214844\n",
      "164 5.788894176483154\n",
      "165 5.554879188537598\n",
      "166 5.32981014251709\n",
      "167 5.113027572631836\n",
      "168 4.9043660163879395\n",
      "169 4.703609943389893\n",
      "170 4.510270595550537\n",
      "171 4.3244123458862305\n",
      "172 4.145694255828857\n",
      "173 3.973907709121704\n",
      "174 3.808793544769287\n",
      "175 3.649976968765259\n",
      "176 3.4975221157073975\n",
      "177 3.350806713104248\n",
      "178 3.209944725036621\n",
      "179 3.07472562789917\n",
      "180 2.9448909759521484\n",
      "181 2.820255994796753\n",
      "182 2.700651168823242\n",
      "183 2.5857601165771484\n",
      "184 2.4755005836486816\n",
      "185 2.3697211742401123\n",
      "186 2.268235921859741\n",
      "187 2.1708364486694336\n",
      "188 2.0773415565490723\n",
      "189 1.9877302646636963\n",
      "190 1.9016839265823364\n",
      "191 1.8190313577651978\n",
      "192 1.7397115230560303\n",
      "193 1.6634151935577393\n",
      "194 1.5900731086730957\n",
      "195 1.5195910930633545\n",
      "196 1.451889991760254\n",
      "197 1.3869000673294067\n",
      "198 1.3244669437408447\n",
      "199 1.264617681503296\n",
      "200 1.207249402999878\n",
      "201 1.1523118019104004\n",
      "202 1.099609375\n",
      "203 1.049180269241333\n",
      "204 1.0009009838104248\n",
      "205 0.954687237739563\n",
      "206 0.9104112386703491\n",
      "207 0.8680629730224609\n",
      "208 0.8276066184043884\n",
      "209 0.7888938188552856\n",
      "210 0.7518959045410156\n",
      "211 0.7165466547012329\n",
      "212 0.6827825903892517\n",
      "213 0.6505382657051086\n",
      "214 0.6197524070739746\n",
      "215 0.5903838872909546\n",
      "216 0.5623257160186768\n",
      "217 0.5355950593948364\n",
      "218 0.5101187825202942\n",
      "219 0.4857913851737976\n",
      "220 0.46259865164756775\n",
      "221 0.44049689173698425\n",
      "222 0.41943982243537903\n",
      "223 0.3993733525276184\n",
      "224 0.38023698329925537\n",
      "225 0.3620159327983856\n",
      "226 0.34465742111206055\n",
      "227 0.32812735438346863\n",
      "228 0.31238436698913574\n",
      "229 0.2973955571651459\n",
      "230 0.28311997652053833\n",
      "231 0.2695314288139343\n",
      "232 0.25661179423332214\n",
      "233 0.24429698288440704\n",
      "234 0.23258338868618011\n",
      "235 0.22144176065921783\n",
      "236 0.21083685755729675\n",
      "237 0.20076049864292145\n",
      "238 0.19117675721645355\n",
      "239 0.18205320835113525\n",
      "240 0.17338554561138153\n",
      "241 0.16513438522815704\n",
      "242 0.15728697180747986\n",
      "243 0.1498246043920517\n",
      "244 0.1427265703678131\n",
      "245 0.13597933948040009\n",
      "246 0.1295572817325592\n",
      "247 0.12345725297927856\n",
      "248 0.11765684932470322\n",
      "249 0.11214020848274231\n",
      "250 0.10689270496368408\n",
      "251 0.10190672427415848\n",
      "252 0.0971565991640091\n",
      "253 0.0926438719034195\n",
      "254 0.08835189789533615\n",
      "255 0.08427232503890991\n",
      "256 0.08039075881242752\n",
      "257 0.07669688016176224\n",
      "258 0.07318565249443054\n",
      "259 0.06984738260507584\n",
      "260 0.06667213886976242\n",
      "261 0.06365060806274414\n",
      "262 0.0607772096991539\n",
      "263 0.05804499238729477\n",
      "264 0.055445652455091476\n",
      "265 0.05297071859240532\n",
      "266 0.050617679953575134\n",
      "267 0.04837964102625847\n",
      "268 0.04624912887811661\n",
      "269 0.044222865253686905\n",
      "270 0.042294032871723175\n",
      "271 0.040458012372255325\n",
      "272 0.03871149569749832\n",
      "273 0.03704887256026268\n",
      "274 0.03546638414263725\n",
      "275 0.03396068140864372\n",
      "276 0.032530706375837326\n",
      "277 0.03116205707192421\n",
      "278 0.029861930757761\n",
      "279 0.02862399071455002\n",
      "280 0.027445461601018906\n",
      "281 0.02632262371480465\n",
      "282 0.025252625346183777\n",
      "283 0.02423342689871788\n",
      "284 0.023261677473783493\n",
      "285 0.022335421293973923\n",
      "286 0.021452441811561584\n",
      "287 0.020610783249139786\n",
      "288 0.019808078184723854\n",
      "289 0.019042907282710075\n",
      "290 0.018312249332666397\n",
      "291 0.01761552132666111\n",
      "292 0.016950499266386032\n",
      "293 0.016315769404172897\n",
      "294 0.01571037247776985\n",
      "295 0.015131337568163872\n",
      "296 0.014578637667000294\n",
      "297 0.014050759375095367\n",
      "298 0.013546379283070564\n",
      "299 0.013064343482255936\n",
      "300 0.01260340865701437\n",
      "301 0.0121627077460289\n",
      "302 0.011740959249436855\n",
      "303 0.01133760716766119\n",
      "304 0.010951558127999306\n",
      "305 0.01058204285800457\n",
      "306 0.010228198952972889\n",
      "307 0.009889226406812668\n",
      "308 0.009564484469592571\n",
      "309 0.009253195486962795\n",
      "310 0.008954991586506367\n",
      "311 0.008668619208037853\n",
      "312 0.008394213393330574\n",
      "313 0.008130832575261593\n",
      "314 0.007877900265157223\n",
      "315 0.0076351347379386425\n",
      "316 0.007401803508400917\n",
      "317 0.007177583407610655\n",
      "318 0.006962063256651163\n",
      "319 0.006754784379154444\n",
      "320 0.006555383559316397\n",
      "321 0.006363494321703911\n",
      "322 0.006178795825690031\n",
      "323 0.006001071538776159\n",
      "324 0.00582959083840251\n",
      "325 0.005664365366101265\n",
      "326 0.00550519023090601\n",
      "327 0.005351655185222626\n",
      "328 0.005203528329730034\n",
      "329 0.0050606499426066875\n",
      "330 0.004922523628920317\n",
      "331 0.0047892373986542225\n",
      "332 0.004660415928810835\n",
      "333 0.004535979591310024\n",
      "334 0.004415587056428194\n",
      "335 0.004299253225326538\n",
      "336 0.004186568316072226\n",
      "337 0.0040775868110358715\n",
      "338 0.003972042351961136\n",
      "339 0.0038698306307196617\n",
      "340 0.003770803567022085\n",
      "341 0.003674842184409499\n",
      "342 0.003581758588552475\n",
      "343 0.003491526935249567\n",
      "344 0.003404011717066169\n",
      "345 0.0033190669491887093\n",
      "346 0.0032366737723350525\n",
      "347 0.0031566787511110306\n",
      "348 0.0030789931770414114\n",
      "349 0.0030035136733204126\n",
      "350 0.002930162474513054\n",
      "351 0.002858852269127965\n",
      "352 0.0027895381208509207\n",
      "353 0.002722103614360094\n",
      "354 0.0026565284933894873\n",
      "355 0.0025927177630364895\n",
      "356 0.002530595287680626\n",
      "357 0.002470139181241393\n",
      "358 0.002411278197541833\n",
      "359 0.002353949472308159\n",
      "360 0.0022981048095971346\n",
      "361 0.0022436981089413166\n",
      "362 0.0021906939800828695\n",
      "363 0.0021390218753367662\n",
      "364 0.0020886536221951246\n",
      "365 0.002039556158706546\n",
      "366 0.0019916596356779337\n",
      "367 0.0019449503161013126\n",
      "368 0.0018993926933035254\n",
      "369 0.0018549625528976321\n",
      "370 0.0018115746788680553\n",
      "371 0.0017692547990009189\n",
      "372 0.0017279447056353092\n",
      "373 0.0016876193694770336\n",
      "374 0.001648259349167347\n",
      "375 0.0016098334453999996\n",
      "376 0.0015723133692517877\n",
      "377 0.0015356560470536351\n",
      "378 0.001499878359027207\n",
      "379 0.0014649234944954515\n",
      "380 0.0014307830715551972\n",
      "381 0.001397431013174355\n",
      "382 0.0013648553285747766\n",
      "383 0.001333025167696178\n",
      "384 0.0013019146863371134\n",
      "385 0.001271539251320064\n",
      "386 0.0012418428668752313\n",
      "387 0.0012128244852647185\n",
      "388 0.0011844661785289645\n",
      "389 0.0011567515321075916\n",
      "390 0.0011296749580651522\n",
      "391 0.0011032066540792584\n",
      "392 0.0010773325338959694\n",
      "393 0.0010520415380597115\n",
      "394 0.0010273290099576116\n",
      "395 0.0010031618876382709\n",
      "396 0.0009795515798032284\n",
      "397 0.0009564661886543036\n",
      "398 0.0009339110110886395\n",
      "399 0.0009118723683059216\n",
      "400 0.0008903192938305438\n",
      "401 0.0008692512637935579\n",
      "402 0.0008486676379106939\n",
      "403 0.0008285325602628291\n",
      "404 0.0008088548784144223\n",
      "405 0.0007896139868535101\n",
      "406 0.0007708189077675343\n",
      "407 0.0007524371612817049\n",
      "408 0.0007344827172346413\n",
      "409 0.0007169188465923071\n",
      "410 0.0006997511954978108\n",
      "411 0.000682971382047981\n",
      "412 0.0006665767286904156\n",
      "413 0.0006505447090603411\n",
      "414 0.0006348786992020905\n",
      "415 0.0006195663008838892\n",
      "416 0.0006045997724868357\n",
      "417 0.0005899662501178682\n",
      "418 0.0005756786558777094\n",
      "419 0.0005617112619802356\n",
      "420 0.0005480517284013331\n",
      "421 0.0005347075057215989\n",
      "422 0.0005216759745962918\n",
      "423 0.0005089393234811723\n",
      "424 0.0004964896361343563\n",
      "425 0.00048432257608510554\n",
      "426 0.0004724416940007359\n",
      "427 0.00046082789776846766\n",
      "428 0.0004494806344155222\n",
      "429 0.00043839853606186807\n",
      "430 0.00042756643961183727\n",
      "431 0.00041699138819240034\n",
      "432 0.00040665941196493804\n",
      "433 0.0003965637006331235\n",
      "434 0.00038670271169394255\n",
      "435 0.00037707723095081747\n",
      "436 0.00036766755511052907\n",
      "437 0.00035848497645929456\n",
      "438 0.0003495095297694206\n",
      "439 0.00034074592986144125\n",
      "440 0.0003321923431940377\n",
      "441 0.00032383459620177746\n",
      "442 0.0003156766761094332\n",
      "443 0.00030771069577895105\n",
      "444 0.00029992996132932603\n",
      "445 0.0002923348802141845\n",
      "446 0.0002849219599738717\n",
      "447 0.00027768348809331656\n",
      "448 0.0002706160012166947\n",
      "449 0.0002637202851474285\n",
      "450 0.0002569903736002743\n",
      "451 0.00025041509070433676\n",
      "452 0.00024399900576099753\n",
      "453 0.0002377383498242125\n",
      "454 0.00023162696743384004\n",
      "455 0.0002256634907098487\n",
      "456 0.00021984528575558215\n",
      "457 0.00021416344679892063\n",
      "458 0.00020862222299911082\n",
      "459 0.0002032156626228243\n",
      "460 0.0001979424268938601\n",
      "461 0.00019279666594229639\n",
      "462 0.0001877708564279601\n",
      "463 0.0001828753447625786\n",
      "464 0.00017809285782277584\n",
      "465 0.00017343356739729643\n",
      "466 0.00016888868412934244\n",
      "467 0.0001644528965698555\n",
      "468 0.00016012620471883565\n",
      "469 0.00015590881230309606\n",
      "470 0.00015179631009232253\n",
      "471 0.0001477810728829354\n",
      "472 0.0001438715698895976\n",
      "473 0.00014005583943799138\n",
      "474 0.00013633459457196295\n",
      "475 0.000132707878947258\n",
      "476 0.0001291694352403283\n",
      "477 0.0001257237745448947\n",
      "478 0.00012236132170073688\n",
      "479 0.00011908815940842032\n",
      "480 0.00011589269706746563\n",
      "481 0.00011278153397142887\n",
      "482 0.00010974630276905373\n",
      "483 0.00010678616672521457\n",
      "484 0.00010390483657829463\n",
      "485 0.0001010964551824145\n",
      "486 9.836108074523509e-05\n",
      "487 9.569292888045311e-05\n",
      "488 9.309493179898709e-05\n",
      "489 9.055839473148808e-05\n",
      "490 8.809468272374943e-05\n",
      "491 8.568834891775623e-05\n",
      "492 8.334738959092647e-05\n",
      "493 8.106733730528504e-05\n",
      "494 7.88411998655647e-05\n",
      "495 7.667910540476441e-05\n",
      "496 7.456810999428853e-05\n",
      "497 7.251522038131952e-05\n",
      "498 7.051113789202645e-05\n",
      "499 6.856440450064838e-05\n"
     ]
    }
   ],
   "source": [
    "N,D_in,H,D_out = 64,1000,100,10\n",
    "\n",
    "x = torch.randn(N,D_in)\n",
    "y = torch.randn(N,D_out)\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self,D_in,H,D_out):\n",
    "        super(TwoLayerNet,self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in,H,bias=False)\n",
    "        self.linear2 = torch.nn.Linear(H,D_out,bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y_pred = self.linear2(self.linear1(x).clamp(min=0))\n",
    "        return y_pred\n",
    "\n",
    "model = TwoLayerNet(D_in,H,D_out)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "for i in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred,y)\n",
    "    print(i,loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
